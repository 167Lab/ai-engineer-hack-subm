from django.shortcuts import render
from rest_framework.response import Response
from rest_framework.views import APIView
from rest_framework.exceptions import ValidationError
from .serializers import (
    DataSourceAnalysisRequestSer, DAGGenerationRequestSer,
    DataSourceAnalysisResponseSer, DAGDeploymentRequestSer
)
from apps.agents.integration import LLMIntegration
from services.airflow import render_dag_py, deploy_dag_to_airflow, get_recs_for_source, delete_dag_properly

# Create your views here.

# /api/v1/analyze
class AnalyzeDataSourceView(APIView):
    def post(self, request):
        import asyncio
        
        ser = DataSourceAnalysisRequestSer(data=request.data)
        ser.is_valid(raise_exception=True)
        
        try:
            mas = LLMIntegration()
            # –ó–∞–ø—É—Å–∫–∞–µ–º async —Ñ—É–Ω–∫—Ü–∏—é –≤ event loop
            result = asyncio.run(mas.analyze_data_source(ser.validated_data))
            return Response(result)
        except Exception as e:
            return Response({
                'error': str(e),
                'status': 'failed'
            }, status=400)

# /api/v1/analyze_file_stream
class AnalyzeFileStreamView(APIView):
    """
    Production-ready streaming file upload –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –ø–æ —á–∞—Å—Ç—è–º, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—è –≤—Å–µ –≤ –ø–∞–º—è—Ç—å
    """
    def post(self, request):
        """
        Streaming –∞–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞
        
        –û–∂–∏–¥–∞–µ—Ç multipart/form-data:
        - file: —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        - source_type: —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (csv, json, xml)
        - sample_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
        """
        import tempfile
        import os
        import logging
        import asyncio
        from django.core.files.storage import default_storage
        from django.core.files.base import ContentFile
        
        logger = logging.getLogger(__name__)
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º—ã–π —Ñ–∞–π–ª
            uploaded_file = request.FILES.get('file')
            if not uploaded_file:
                return Response({
                    'error': '–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ',
                    'status': 'failed'
                }, status=400)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            source_type = request.data.get('source_type', 'csv').lower()
            sample_size = int(request.data.get('sample_size', 1000))
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
            max_size = 1024 * 1024 * 1024  # 1 GB
            if uploaded_file.size > max_size:
                return Response({
                    'error': f'–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º—É–º: {max_size // (1024*1024)} MB',
                    'details': {
                        'file_size': uploaded_file.size,
                        'max_size': max_size,
                        'file_name': uploaded_file.name
                    },
                    'status': 'failed'
                }, status=413)
            
            logger.info(f"üìÅ Streaming –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {uploaded_file.name} ({uploaded_file.size} bytes)")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è streaming –æ–±—Ä–∞–±–æ—Ç–∫–∏
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{source_type}') as tmp_file:
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º (chunks) —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤ –ø–∞–º—è—Ç—å
                chunk_size = 64 * 1024  # 64KB chunks
                total_written = 0
                
                for chunk in uploaded_file.chunks(chunk_size):
                    tmp_file.write(chunk)
                    total_written += len(chunk)
                
                tmp_file.flush()
                logger.info(f"üìù –ó–∞–ø–∏—Å–∞–Ω–æ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {total_written} bytes")
                
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (Stack Overflow + –Ω–∞—à–∏ —É–ª—É—á—à–µ–Ω–∏—è)
                    from analyzers.hybrid_file_analyzer import HybridFileAnalyzer
                    
                    analyzer = HybridFileAnalyzer()
                    result = analyzer.analyze_uploaded_file(
                        file_path=tmp_file.name,
                        source_type=source_type,
                        sample_size=sample_size,
                        original_filename=uploaded_file.name
                    )
                    
                    logger.info(f"‚úÖ Streaming –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è —Ñ–∞–π–ª–∞ {uploaded_file.name}")
                    
                    return Response({
                        'status': 'success',
                        'analysis_result': result,
                        'file_info': {
                            'name': uploaded_file.name,
                            'size': uploaded_file.size,
                            'processed_size': total_written,
                            'source_type': source_type,
                            'sample_size': sample_size
                        }
                    })
                    
                except Exception as analysis_error:
                    logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {uploaded_file.name}: {analysis_error}")
                    
                    return Response({
                        'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞: {str(analysis_error)}',
                        'details': {
                            'file_name': uploaded_file.name,
                            'analysis_error': str(analysis_error)
                        },
                        'status': 'failed'
                    }, status=500)
                
                finally:
                    # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    try:
                        os.unlink(tmp_file.name)
                        logger.info(f"üßπ –£–¥–∞–ª–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {tmp_file.name}")
                    except Exception as cleanup_error:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {cleanup_error}")
        
        except Exception as e:
            logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ streaming upload: {e}")
            
            return Response({
                'error': f'–°–∏—Å—Ç–µ–º–Ω–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}',
                'status': 'failed'
            }, status=500)

# /api/v1/generate_dag
class GenerateDAGView(APIView):
    def post(self, request):
        ser = DAGGenerationRequestSer(data=request.data)
        ser.is_valid(raise_exception=True)
        dag_py, dag_id = render_dag_py(ser.validated_data)
        return Response({"dag_id": dag_id, "dag_py": dag_py})

# /api/v1/recommendations?source_id=...
class GetRecommendationsView(APIView):
    def get(self, request):
        source_id = request.query_params.get("source_id")
        if not source_id:
            raise ValidationError({"source_id": "This query parameter is required"})
        recs = get_recs_for_source(source_id)
        return Response({"source_id": source_id, "recommendations": recs})

# /api/v1/deploy_dag
class DeployDAGView(APIView):
    def post(self, request):
        ser = DAGDeploymentRequestSer(data=request.data)
        ser.is_valid(raise_exception=True)
        deploy_info = deploy_dag_to_airflow(ser.validated_data)
        return Response(deploy_info)

# /api/v1/delete_dag_complete
class DeleteDAGCompleteView(APIView):
    """
    Production-ready endpoint –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è DAG
    –£–¥–∞–ª—è–µ—Ç DAG –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö Airflow –ò —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π —Ñ–∞–π–ª
    """
    def delete(self, request, dag_id):
        """
        –ü–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ DAG –ø–æ dag_id
        
        Args:
            dag_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä DAG –∏–∑ URL path
            
        Returns:
            JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è
        """
        import logging
        
        logger = logging.getLogger(__name__)
        logger.info(f"üóëÔ∏è –ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ DAG: {dag_id}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è dag_id
        if not dag_id or not isinstance(dag_id, str):
            return Response({
                "status": "error",
                "message": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π dag_id",
                "details": {"dag_id": dag_id}
            }, status=400)
        
        # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è dag_id (–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å)
        dag_id = dag_id.strip()
        if not dag_id.replace('_', '').replace('-', '').isalnum():
            return Response({
                "status": "error", 
                "message": "DAG ID —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã",
                "details": {"allowed": "–±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, _, -"}
            }, status=400)
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º —É–¥–∞–ª–µ–Ω–∏–µ
            result = delete_dag_properly(dag_id)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if result["status"] == "success":
                logger.info(f"‚úÖ DAG '{dag_id}' —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é")
                status_code = 200
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è DAG '{dag_id}': {result.get('message')}")
                status_code = 500
                
            return Response(result, status=status_code)
            
        except Exception as e:
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - –ª–æ–≥–∏—Ä—É–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ
            logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è DAG '{dag_id}': {e}")
            
            return Response({
                "status": "error",
                "message": f"–°–∏—Å—Ç–µ–º–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ DAG '{dag_id}'",
                "details": {
                    "exception": str(e),
                    "dag_id": dag_id
                }
            }, status=500)

# /api/v1/dags/cleanup_orphaned
class CleanupOrphanedDAGsView(APIView):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö DAG —Ñ–∞–π–ª–æ–≤
    (—Ñ–∞–π–ª—ã –µ—Å—Ç—å, –Ω–æ DAG –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ Airflow)
    """
    def post(self, request):
        """
        –ü–æ–∏—Å–∫ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤ DAG
        
        Query params:
            dry_run: –µ—Å–ª–∏ true, —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ
        """
        import logging
        from generators.dag_cleanup_utils import DAGManager
        
        logger = logging.getLogger(__name__)
        dry_run = request.query_params.get('dry_run', 'false').lower() == 'true'
        
        logger.info(f"üßπ –ó–∞–ø—Ä–æ—Å –æ—á–∏—Å—Ç–∫–∏ –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö DAG (dry_run={dry_run})")
        
        try:
            manager = DAGManager()
            
            # –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏–º –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏–µ —Ñ–∞–π–ª—ã
            orphaned_files = manager.list_orphaned_files()
            
            if not orphaned_files:
                return Response({
                    "status": "success",
                    "message": "–û—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                    "details": {
                        "orphaned_count": 0,
                        "files_deleted": 0,
                        "dry_run": dry_run
                    }
                })
            
            if dry_run:
                # –†–µ–∂–∏–º –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ - –Ω–µ —É–¥–∞–ª—è–µ–º
                logger.info(f"üîç [DRY RUN] –ù–∞–π–¥–µ–Ω–æ {len(orphaned_files)} –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤")
                return Response({
                    "status": "success",
                    "message": f"[DRY RUN] –ù–∞–π–¥–µ–Ω–æ {len(orphaned_files)} –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤",
                    "details": {
                        "orphaned_files": orphaned_files,
                        "orphaned_count": len(orphaned_files),
                        "would_delete": len(orphaned_files),
                        "dry_run": True
                    }
                })
            
            # –†–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è
            deleted_count = manager.cleanup_all_orphaned()
            
            logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ {len(orphaned_files)}")
            
            return Response({
                "status": "success",
                "message": f"–£–¥–∞–ª–µ–Ω–æ {deleted_count} –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤",
                "details": {
                    "orphaned_files": orphaned_files,
                    "orphaned_count": len(orphaned_files),
                    "files_deleted": deleted_count,
                    "dry_run": False
                }
            })
            
        except Exception as e:
            logger.exception(f"üí• –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤: {e}")
            
            return Response({
                "status": "error",
                "message": "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤",
                "details": {
                    "exception": str(e),
                    "dry_run": dry_run
                }
            }, status=500)

# /api/v1/dags/health_report
class DAGHealthReportView(APIView):
    """
    –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è DAG
    """
    def get(self, request):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã
        
        Query params:
            hours: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 24)
        """
        import logging
        from services.dag_monitoring import get_monitoring_service
        
        logger = logging.getLogger(__name__)
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            hours = int(request.query_params.get('hours', '24'))
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if hours < 1 or hours > 168:  # –û—Ç 1 —á–∞—Å–∞ –¥–æ 7 –¥–Ω–µ–π
                return Response({
                    "status": "error",
                    "message": "–ü–∞—Ä–∞–º–µ—Ç—Ä hours –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 168 (7 –¥–Ω–µ–π)",
                    "details": {"provided_hours": hours}
                }, status=400)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Ä–≤–∏—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            monitoring = get_monitoring_service()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            health_report = monitoring.get_health_report(hours=hours)
            
            logger.info(f"üìä –°–æ–∑–¥–∞–Ω health report –∑–∞ {hours} —á–∞—Å–æ–≤, —Å—Ç–∞—Ç—É—Å: {health_report.get('status')}")
            
            return Response(health_report)
            
        except ValueError:
            return Response({
                "status": "error",
                "message": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ hours",
                "details": {"hours": request.query_params.get('hours')}
            }, status=400)
            
        except Exception as e:
            logger.exception(f"üí• –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è health report: {e}")
            
            return Response({
                "status": "error",
                "message": "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏",
                "details": {"exception": str(e)}
            }, status=500)

# /api/v1/upload_chunk
class UploadChunkView(APIView):
    """
    Endpoint –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –ø–æ —á–∞—Å—Ç—è–º (chunks)
    –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ —Å–µ—Ä–≤–µ—Ä–∞
    """
    def post(self, request):
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —Ñ–∞–π–ª–∞
        
        Form data:
        - chunk: —á–∞—Å—Ç—å —Ñ–∞–π–ª–∞ (Blob)
        - upload_id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–≥—Ä—É–∑–∫–∏
        - chunk_index: –∏–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞
        - total_chunks: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        - file_name: –∏–º—è —Ñ–∞–π–ª–∞
        - source_type: —Ç–∏–ø —Ñ–∞–π–ª–∞
        - chunk_hash: —Ö–µ—à –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        """
        import os
        import logging
        from django.conf import settings
        
        logger = logging.getLogger(__name__)
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
            chunk = request.FILES.get('chunk')
            if not chunk:
                return Response({
                    'error': '–ß–∞–Ω–∫ —Ñ–∞–π–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω',
                    'status': 'failed'
                }, status=400)
            
            upload_id = request.data.get('upload_id')
            chunk_index = int(request.data.get('chunk_index', 0))
            total_chunks = int(request.data.get('total_chunks', 1))
            file_name = request.data.get('file_name')
            source_type = request.data.get('source_type')
            chunk_hash = request.data.get('chunk_hash')
            
            logger.info(f"üì¶ –ü–æ–ª—É—á–µ–Ω —á–∞–Ω–∫ {chunk_index + 1}/{total_chunks} –¥–ª—è —Ñ–∞–π–ª–∞ {file_name}")
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏
            upload_dir = os.path.join(settings.FILE_UPLOAD_TEMP_DIR or '/tmp', 'chunked_uploads', upload_id)
            os.makedirs(upload_dir, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫
            chunk_path = os.path.join(upload_dir, f'chunk_{chunk_index:04d}')
            with open(chunk_path, 'wb') as f:
                for chunk_data in chunk.chunks():
                    f.write(chunk_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
            metadata_path = os.path.join(upload_dir, f'chunk_{chunk_index:04d}.meta')
            with open(metadata_path, 'w') as f:
                import json
                json.dump({
                    'index': chunk_index,
                    'total_chunks': total_chunks,
                    'file_name': file_name,
                    'source_type': source_type,
                    'chunk_hash': chunk_hash,
                    'size': chunk.size
                }, f)
            
            logger.info(f"‚úÖ –ß–∞–Ω–∫ {chunk_index + 1}/{total_chunks} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {chunk.size} bytes")
            
            return Response({
                'status': 'success',
                'message': f'–ß–∞–Ω–∫ {chunk_index + 1}/{total_chunks} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω',
                'details': {
                    'upload_id': upload_id,
                    'chunk_index': chunk_index,
                    'chunk_size': chunk.size,
                    'progress': round(((chunk_index + 1) / total_chunks) * 100, 1)
                }
            })
            
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–∞: {e}")
            return Response({
                'error': f'–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–∞: {str(e)}',
                'status': 'failed'
            }, status=500)

# /api/v1/finalize_chunked_upload  
class FinalizeChunkedUploadView(APIView):
    """
    Endpoint –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –∏ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
    """
    def post(self, request):
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç streaming –∞–Ω–∞–ª–∏–∑
        """
        import os
        import logging
        import json
        import asyncio
        from django.conf import settings
        
        logger = logging.getLogger(__name__)
        
        try:
            data = request.data
            upload_id = data.get('upload_id')
            file_name = data.get('file_name')
            source_type = data.get('source_type')
            file_size = data.get('file_size', 0)
            sample_size = int(data.get('sample_size', 1000))
            
            logger.info(f"üîó –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è chunked upload: {file_name} ({file_size} bytes)")
            
            upload_dir = os.path.join(settings.FILE_UPLOAD_TEMP_DIR or '/tmp', 'chunked_uploads', upload_id)
            
            if not os.path.exists(upload_dir):
                return Response({
                    'error': f'–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {upload_id}',
                    'status': 'failed'
                }, status=404)
            
            # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–∞—Ö
            chunks_info = []
            for file in os.listdir(upload_dir):
                if file.endswith('.meta'):
                    with open(os.path.join(upload_dir, file), 'r') as f:
                        chunks_info.append(json.load(f))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É
            chunks_info.sort(key=lambda x: x['index'])
            
            if not chunks_info:
                return Response({
                    'error': '–ß–∞–Ω–∫–∏ —Ñ–∞–π–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã',
                    'status': 'failed'
                }, status=400)
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{source_type}') as combined_file:
                combined_path = combined_file.name
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏
                total_size = 0
                for chunk_info in chunks_info:
                    chunk_path = os.path.join(upload_dir, f'chunk_{chunk_info["index"]:04d}')
                    if os.path.exists(chunk_path):
                        with open(chunk_path, 'rb') as chunk_file:
                            data_chunk = chunk_file.read()
                            combined_file.write(data_chunk)
                            total_size += len(data_chunk)
                
                logger.info(f"üìÅ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(chunks_info)} —á–∞–Ω–∫–æ–≤ –≤ —Ñ–∞–π–ª: {total_size} bytes")
            
            try:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                from analyzers.hybrid_file_analyzer import HybridFileAnalyzer
                
                analyzer = HybridFileAnalyzer()
                result = analyzer.analyze_uploaded_file(
                    file_path=combined_path,
                    source_type=source_type,
                    sample_size=sample_size,
                    original_filename=file_name
                )
                
                logger.info(f"‚úÖ Chunked –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è —Ñ–∞–π–ª–∞ {file_name}")
                
                return Response({
                    'status': 'success',
                    'analysis_result': result,
                    'file_info': {
                        'name': file_name,
                        'size': file_size,
                        'processed_size': total_size,
                        'source_type': source_type,
                        'sample_size': sample_size,
                        'chunks_processed': len(chunks_info)
                    }
                })
                
            finally:
                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                try:
                    os.unlink(combined_path)
                    logger.info(f"üßπ –£–¥–∞–ª–µ–Ω –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {combined_path}")
                except:
                    pass
                
                # –û—á–∏—â–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —á–∞–Ω–∫–∞–º–∏
                try:
                    import shutil
                    shutil.rmtree(upload_dir)
                    logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —á–∞–Ω–∫–æ–≤: {upload_dir}")
                except:
                    pass
                    
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ chunked upload: {e}")
            return Response({
                'error': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}',
                'status': 'failed'
            }, status=500)

# /api/v1/cleanup_upload
class CleanupUploadView(APIView):
    """
    Endpoint –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
    """
    def delete(self, request):
        """–û—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ—É–¥–∞—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
        import os
        import logging
        import shutil
        from django.conf import settings
        
        logger = logging.getLogger(__name__)
        
        try:
            upload_id = request.data.get('upload_id')
            if not upload_id:
                return Response({
                    'error': 'upload_id –Ω–µ —É–∫–∞–∑–∞–Ω',
                    'status': 'failed'
                }, status=400)
            
            upload_dir = os.path.join(settings.FILE_UPLOAD_TEMP_DIR or '/tmp', 'chunked_uploads', upload_id)
            
            if os.path.exists(upload_dir):
                shutil.rmtree(upload_dir)
                logger.info(f"üßπ –û—á–∏—â–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ—É–¥–∞—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {upload_id}")
            
            return Response({
                'status': 'success',
                'message': f'–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏ {upload_id} –æ—á–∏—â–µ–Ω—ã'
            })
            
        except Exception as e:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return Response({
                'error': f'–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {str(e)}',
                'status': 'failed'
            }, status=500)