
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import pandas as pd
import logging
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def extract_data():
    '''–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ csv'''
    import pandas as pd
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ csv")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        if not os.path.exists('/opt/airflow/data/sample.csv'):
            raise FileNotFoundError(f"–§–∞–π–ª –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: /opt/airflow/data/sample.csv")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if 'csv' == 'csv':
            df = pd.read_csv('/opt/airflow/data/sample.csv')
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ CSV —Ñ–∞–π–ª–∞")
            
        elif 'csv' == 'json':
            df = pd.read_json('/opt/airflow/data/sample.csv')
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ JSON —Ñ–∞–π–ª–∞")
            
        elif 'csv' == 'xml':
            df = pd.read_xml('/opt/airflow/data/sample.csv')
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ XML —Ñ–∞–π–ª–∞")
            
        elif 'csv' == 'parquet':
            df = pd.read_parquet('/opt/airflow/data/sample.csv')
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ Parquet —Ñ–∞–π–ª–∞")
            
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: csv")
        
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        if df.empty:
            raise ValueError("–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –ø—É—Å—Ç")
            
        logger.info(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"   - –°—Ç—Ä–æ–∫: {len(df)}")
        logger.info(f"   - –ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
        logger.info(f"   - –ö–æ–ª–æ–Ω–∫–∏: {', '.join(df.columns)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        temp_dir = '/opt/airflow/data/temp'
        os.makedirs(temp_dir, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ
        temp_path = '/opt/airflow/data/temp/{dag_id}_extracted.parquet'
        df.to_parquet(temp_path, index=False)
        logger.info("üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: %s" % temp_path)
        
        return temp_path
        
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: %s" % str(e))
        raise



def transform_data():
    '''–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö'''
    import pandas as pd
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    logger.info("üîÑ –ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    try:
        temp_path = '/opt/airflow/data/temp/{{dag_id}}_extracted.parquet'
        
        if not os.path.exists(temp_path):
            raise FileNotFoundError(f"–§–∞–π–ª —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {temp_path}")
            
        df = pd.read_parquet(temp_path)
        initial_rows = len(df)
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {initial_rows} —Å—Ç—Ä–æ–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        transformation_steps = []
        
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        null_counts = df.isnull().sum()
        if null_counts.sum() > 0:
            logger.info(f"üßπ –ù–∞–π–¥–µ–Ω–æ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {null_counts.sum()}")
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —Ç–∏–ø–∞–º –∫–æ–ª–æ–Ω–æ–∫
            for col in df.columns:
                if df[col].dtype in ['int64', 'float64']:
                    df[col].fillna(df[col].mean(), inplace=True)
                else:
                    df[col].fillna('Unknown', inplace=True)
            
            transformation_steps.append(f"–ó–∞–ø–æ–ª–Ω–µ–Ω—ã –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        
        # 2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            df = df.drop_duplicates()
            transformation_steps.append(f"–£–¥–∞–ª–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            logger.info(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        # 3. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –≥–¥–µ —ç—Ç–æ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª
        string_columns = df.select_dtypes(include=['object']).columns
        for col in string_columns:
            if col.lower() in ['email', 'city', 'department']:
                df[col] = df[col].astype(str).str.strip().str.title()
                transformation_steps.append(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ {col}")
        
        # 4. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        df['processed_at'] = pd.Timestamp.now()
        df['processing_batch_id'] = '{{dag_id}}'
        transformation_steps.append("–î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        final_rows = len(df)
        logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {initial_rows} ‚Üí {final_rows} —Å—Ç—Ä–æ–∫")
        
        if transformation_steps:
            logger.info("üîß –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:")
            for step in transformation_steps:
                logger.info(f"   - {step}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        transformed_path = '/opt/airflow/data/temp/{{dag_id}}_transformed.parquet'
        df.to_parquet(transformed_path, index=False)
        logger.info(f"üíæ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {transformed_path}")
        
        return transformed_path
        
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: %s" % str(e))
        raise


def load_data():
    '''–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ postgres'''
    import pandas as pd
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    logger.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ postgres")
    
    transformed_path = '/opt/airflow/data/temp/{dag_id}_transformed.parquet'
    
    if not os.path.exists(transformed_path):
        raise FileNotFoundError(f"–§–∞–π–ª —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: /opt/airflow/data/temp/Test_02102025_transformed.parquet")
        
    df = pd.read_parquet(transformed_path)
    logger.info("üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ %d —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ" % len(df))
    
    # –†–µ–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL
    try:
        from sqlalchemy import create_engine
        import os
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        connection_string = os.getenv(
            'POSTGRES_CONNECTION_STRING', 
            "postgresql://airflow:airflow@postgres:5432/etl_data"
        )
        
        engine = create_engine(connection_string)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É –≤ —Å—Ö–µ–º–µ processed
        df.to_sql(
            'processed_data', 
            engine, 
            schema='processed',
            if_exists='append',  # –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ 'replace' –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–º–µ–Ω—ã
            index=False,
            method='multi'  # –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        )
        
        row_count = len(df)
        logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ %d —Å—Ç—Ä–æ–∫ –≤ PostgreSQL —Ç–∞–±–ª–∏—Ü—É processed.processed_data" % row_count)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É
        with engine.connect() as conn:
            result = conn.execute(f"SELECT COUNT(*) FROM processed.processed_data").fetchone()
            total_rows = result[0]
            logger.info("üìä –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ processed.processed_data: %d" % total_rows)
            
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ PostgreSQL: %s" % str(e))
        raise

    
    logger.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è DAG
default_args = {
    'owner': 'etl-system',
    'depends_on_past': False,
    'start_date': datetime(2025, 9, 27),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ DAG
with DAG(
    'Test_02102025',
    default_args=default_args,
    description='Auto-generated ETL pipeline: Test_02102025',
    schedule_interval='@daily',
    catchup=False,
    tags=['generated', 'etl', 'csv', 'postgres']
) as dag:
    
    # –ó–∞–¥–∞—á–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    extract_task = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data,
        doc_md='''### üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        
        **–ò—Å—Ç–æ—á–Ω–∏–∫**: csv
        **–ü—É—Ç—å**: /opt/airflow/data/sample.csv
        
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞.
        '''
    )
    
    # –ó–∞–¥–∞—á–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö  
    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        doc_md='''### üîß –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—á–∏—Å—Ç–∫—É –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö:
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤  
        - –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤
        - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏
        '''
    )
    
    # –ó–∞–¥–∞—á–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    load_task = PythonOperator(
        task_id='load_data', 
        python_callable=load_data,
        doc_md='''### üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
        **–¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞**: postgres
        **–¢–∞–±–ª–∏—Ü–∞**: processed_data
        
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ü–µ–ª–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.
        '''
    )
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∑–∞–¥–∞—á
    extract_task >> transform_task >> load_task
