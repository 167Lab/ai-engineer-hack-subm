
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import pandas as pd
import logging
import os

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def extract_data():
    '''Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° csv'''
    import pandas as pd
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ”„ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· csv")
    
    try:
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°
        if not os.path.exists('/opt/airflow/data/sample.csv'):
            raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: /opt/airflow/data/sample.csv")
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ°
        if 'csv' == 'csv':
            df = pd.read_csv('/opt/airflow/data/sample.csv')
            logger.info(f"ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· CSV Ñ„Ğ°Ğ¹Ğ»Ğ°")
            
        elif 'csv' == 'json':
            df = pd.read_json('/opt/airflow/data/sample.csv')
            logger.info(f"ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· JSON Ñ„Ğ°Ğ¹Ğ»Ğ°")
            
        elif 'csv' == 'xml':
            df = pd.read_xml('/opt/airflow/data/sample.csv')
            logger.info(f"ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· XML Ñ„Ğ°Ğ¹Ğ»Ğ°")
            
        elif 'csv' == 'parquet':
            df = pd.read_parquet('/opt/airflow/data/sample.csv')
            logger.info(f"ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} ÑÑ‚Ñ€Ğ¾Ğº Ğ¸Ğ· Parquet Ñ„Ğ°Ğ¹Ğ»Ğ°")
            
        else:
            raise ValueError(f"ĞĞµĞ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°: csv")
        
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        if df.empty:
            raise ValueError("Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑÑ‚")
            
        logger.info(f"ğŸ“Š Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:")
        logger.info(f"   - Ğ¡Ñ‚Ñ€Ğ¾Ğº: {len(df)}")
        logger.info(f"   - ĞšĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº: {len(df.columns)}")
        logger.info(f"   - ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {', '.join(df.columns)}")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
        temp_dir = '/opt/airflow/data/temp'
        os.makedirs(temp_dir, exist_ok=True)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ
        temp_path = '/opt/airflow/data/temp/{dag_id}_extracted.parquet'
        df.to_parquet(temp_path, index=False)
        logger.info("ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»: %s" % temp_path)
        
        return temp_path
        
    except Exception as e:
        logger.error("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: %s" % str(e))
        raise



def transform_data():
    '''Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…'''
    import pandas as pd
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ”„ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    
    try:
        temp_path = '/opt/airflow/data/temp/{{dag_id}}_extracted.parquet'
        
        if not os.path.exists(temp_path):
            raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {temp_path}")
            
        df = pd.read_parquet(temp_path)
        initial_rows = len(df)
        logger.info(f"ğŸ“Š Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {initial_rows} ÑÑ‚Ñ€Ğ¾Ğº Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸")
        
        # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        transformation_steps = []
        
        # 1. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿ÑƒÑÑ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
        null_counts = df.isnull().sum()
        if null_counts.sum() > 0:
            logger.info(f"ğŸ§¹ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿ÑƒÑÑ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹: {null_counts.sum()}")
            
            # Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿ÑƒÑÑ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
            for col in df.columns:
                if df[col].dtype in ['int64', 'float64']:
                    df[col].fillna(df[col].mean(), inplace=True)
                else:
                    df[col].fillna('Unknown', inplace=True)
            
            transformation_steps.append(f"Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ñ‹ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ")
        
        # 2. Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            df = df.drop_duplicates()
            transformation_steps.append(f"Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ {duplicates} Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²")
            logger.info(f"ğŸ”„ Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ {duplicates} Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²")
        
        # 3. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        # ĞŸÑ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğº Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼Ñƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñƒ Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ¼Ñ‹ÑĞ»
        string_columns = df.select_dtypes(include=['object']).columns
        for col in string_columns:
            if col.lower() in ['email', 'city', 'department']:
                df[col] = df[col].astype(str).str.strip().str.title()
                transformation_steps.append(f"Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° {col}")
        
        # 4. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        df['processed_at'] = pd.Timestamp.now()
        df['processing_batch_id'] = '{{dag_id}}'
        transformation_steps.append("Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸")
        
        final_rows = len(df)
        logger.info(f"âœ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°: {initial_rows} â†’ {final_rows} ÑÑ‚Ñ€Ğ¾Ğº")
        
        if transformation_steps:
            logger.info("ğŸ”§ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸:")
            for step in transformation_steps:
                logger.info(f"   - {step}")
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        transformed_path = '/opt/airflow/data/temp/{{dag_id}}_transformed.parquet'
        df.to_parquet(transformed_path, index=False)
        logger.info(f"ğŸ’¾ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {transformed_path}")
        
        return transformed_path
        
    except Exception as e:
        logger.error("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: %s" % str(e))
        raise


def load_data():
    '''Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ postgres'''
    import pandas as pd
    import logging
    import os
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ”„ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² postgres")
    
    transformed_path = '/opt/airflow/data/temp/{dag_id}_transformed.parquet'
    
    if not os.path.exists(transformed_path):
        raise FileNotFoundError(f"Ğ¤Ğ°Ğ¹Ğ» Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: /opt/airflow/data/temp/Test_02102025_transformed.parquet")
        
    df = pd.read_parquet(transformed_path)
    logger.info("ğŸ“Š Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ %d ÑÑ‚Ñ€Ğ¾Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ" % len(df))
    
    # Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² PostgreSQL
    try:
        from sqlalchemy import create_engine
        import os
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
        connection_string = os.getenv(
            'POSTGRES_CONNECTION_STRING', 
            "postgresql://airflow:airflow@postgres:5432/etl_data"
        )
        
        engine = create_engine(connection_string)
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ² ÑÑ…ĞµĞ¼Ğµ processed
        df.to_sql(
            'processed_data', 
            engine, 
            schema='processed',
            if_exists='append',  # Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ° 'replace' Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹
            index=False,
            method='multi'  # Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        )
        
        row_count = len(df)
        logger.info("âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ %d ÑÑ‚Ñ€Ğ¾Ğº Ğ² PostgreSQL Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ processed.processed_data" % row_count)
        
        # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ
        with engine.connect() as conn:
            result = conn.execute(f"SELECT COUNT(*) FROM processed.processed_data").fetchone()
            total_rows = result[0]
            logger.info("ğŸ“Š Ğ’ÑĞµĞ³Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ processed.processed_data: %d" % total_rows)
            
    except Exception as e:
        logger.error("âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ² PostgreSQL: %s" % str(e))
        raise

    
    logger.info("âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾")

# ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ DAG
default_args = {
    'owner': 'etl-system',
    'depends_on_past': False,
    'start_date': datetime(2025, 9, 27),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ DAG
with DAG(
    'Test_02102025',
    default_args=default_args,
    description='Auto-generated ETL pipeline: Test_02102025',
    schedule_interval='@daily',
    catchup=False,
    tags=['generated', 'etl', 'csv', 'postgres']
) as dag:
    
    # Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    extract_task = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data,
        doc_md='''### ğŸ“¥ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        **Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº**: csv
        **ĞŸÑƒÑ‚ÑŒ**: /opt/airflow/data/sample.csv
        
        Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.
        '''
    )
    
    # Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…  
    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        doc_md='''### ğŸ”§ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:
        - ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿ÑƒÑÑ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
        - Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²  
        - Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²
        - Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        '''
    )
    
    # Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    load_task = PythonOperator(
        task_id='load_data', 
        python_callable=load_data,
        doc_md='''### ğŸ“¤ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        **Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°**: postgres
        **Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°**: processed_data
        
        Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°.
        '''
    )
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡
    extract_task >> transform_task >> load_task
